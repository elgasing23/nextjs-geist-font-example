{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 7, "column": 0}, "map": {"version":3,"file":"optimizer.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-core/src/optimizers/optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {dispose} from '../globals';\nimport {variableGrads} from '../gradients';\nimport {scalar} from '../ops/ops';\nimport {Serializable} from '../serialization';\nimport {Scalar, Variable} from '../tensor';\nimport {NamedTensor, NamedTensorMap} from '../tensor_types';\n\n/**\n * A variable that belongs to an optimizer.\n *\n * The `originalName` field is required for keeping track of the canonical\n * name of the variable, which is usually the name of the model weight that\n * the variable is related to plus a suffix, e.g., 'dense1/kernel/momentum'.\n * The name of the `Variable` object itself cannot be used directly due to\n * possible deduplication: Every `Variable` must have a unique name but more\n * than one optimizer objects of the same type may be created for the same model\n * or the same `Variable`.\n */\nexport interface OptimizerVariable {\n  originalName: string;\n  variable: Variable;\n}\n\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\nexport abstract class Optimizer extends Serializable {\n  protected iterations_: number;\n\n  /**\n   * Executes `f()` and minimizes the scalar output of `f()` by computing\n   * gradients of y with respect to the list of trainable variables provided by\n   * `varList`. If no list is provided, it defaults to all trainable variables.\n   *\n   * @param f The function to execute and whose output to minimize.\n   * @param returnCost Whether to return the scalar cost value produced by\n   * executing `f()`.\n   * @param varList An optional list of variables to update. If specified, only\n   * the trainable variables in varList will be updated by minimize. Defaults to\n   * all trainable variables.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n  minimize(f: () => Scalar, returnCost = false, varList?: Variable[]): Scalar\n      |null {\n    const {value, grads} = this.computeGradients(f, varList);\n\n    if (varList != null) {\n      const gradArray: NamedTensor[] =\n          varList.map(v => ({name: v.name, tensor: grads[v.name]}));\n      this.applyGradients(gradArray);\n    } else {\n      this.applyGradients(grads);\n    }\n\n    // Dispose gradients.\n    dispose(grads);\n\n    if (returnCost) {\n      return value;\n    } else {\n      value.dispose();\n      return null;\n    }\n  }\n\n  /**\n   * The number of iterations that this optimizer instance has been invoked for.\n   */\n  get iterations(): number {\n    if (this.iterations_ == null) {\n      this.iterations_ = 0;\n    }\n    return this.iterations_;\n  }\n\n  protected incrementIterations() {\n    this.iterations_ = this.iterations + 1;\n  }\n\n  /**\n   * Executes f() and computes the gradient of the scalar output of f() with\n   * respect to the list of trainable variables provided by `varList`. If no\n   * list is provided, it defaults to all trainable variables.\n   *\n   * @param f The function to execute and whose output to use for computing\n   * gradients with respect to variables.\n   * @param varList An optional list of variables to compute gradients with\n   * respect to. If specified, only the trainable variables in varList will have\n   * gradients computed with respect to. Defaults to all trainable variables.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n  computeGradients(f: () => Scalar, varList?: Variable[]):\n      {value: Scalar, grads: NamedTensorMap} {\n    return variableGrads(f, varList);\n  }\n\n  /**\n   * Updates variables by using the computed gradients.\n   *\n   * @param variableGradients A mapping of variable name to its gradient value.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n  abstract applyGradients(variableGradients: NamedTensorMap|\n                          NamedTensor[]): void;\n\n  /**\n   * Dispose the variables (if any) owned by this optimizer instance.\n   */\n  dispose(): void {\n    if (this.iterations_ != null) {\n      dispose(this.iterations_);\n    }\n  }\n\n  async saveIterations(): Promise<NamedTensor> {\n    if (this.iterations_ == null) {\n      this.iterations_ = 0;\n    }\n    return {\n      name: 'iter',  // Named for Python compatibility.\n      // TODO(cais): Use 'int64' type when available.\n      tensor: scalar(this.iterations_, 'int32')\n    };\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    throw new Error('getWeights() is not implemented for this optimizer yet.');\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    throw new Error(\n        `setWeights() is not implemented for this optimizer class ` +\n        `${this.getClassName()}`);\n  }\n\n  /**\n   * Extract the first element of the weight values and set it\n   * as the iterations counter variable of this instance of optimizer.\n   *\n   * @param weightValues\n   * @returns Weight values with the first element consumed and excluded.\n   */\n  protected async extractIterations(weightValues: NamedTensor[]):\n      Promise<NamedTensor[]> {\n    this.iterations_ = (await weightValues[0].tensor.data())[0];\n    return weightValues.slice(1);\n  }\n}\n\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n  value: (instance: Optimizer) => {\n    return instance.minimize != null && instance.computeGradients != null &&\n        instance.applyGradients != null;\n  }\n});\n"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;;;AAEH,OAAO,EAAC,OAAO,EAAC,MAAM,YAAY,CAAC;AACnC,OAAO,EAAC,aAAa,EAAC,MAAM,cAAc,CAAC;AAC3C,OAAO,EAAC,MAAM,EAAC,MAAM,YAAY,CAAC;AAClC,OAAO,EAAC,YAAY,EAAC,MAAM,kBAAkB,CAAC;;;;;AAqBxC,MAAgB,SAAU,0NAAQ,eAAY;IAGlD;;;;;;;;;;;;;OAaG,CACH,QAAQ,CAAC,CAAe,EAAE,UAAU,GAAG,KAAK,EAAE,OAAoB,EAAA;QAEhE,MAAM,EAAC,KAAK,EAAE,KAAK,EAAC,GAAG,IAAI,CAAC,gBAAgB,CAAC,CAAC,EAAE,OAAO,CAAC,CAAC;QAEzD,IAAI,OAAO,IAAI,IAAI,EAAE;YACnB,MAAM,SAAS,GACX,OAAO,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC;oBAAC,IAAI,EAAE,CAAC,CAAC,IAAI;oBAAE,MAAM,EAAE,KAAK,CAAC,CAAC,CAAC,IAAI,CAAC;gBAAA,CAAC,CAAC,CAAC,CAAC;YAC9D,IAAI,CAAC,cAAc,CAAC,SAAS,CAAC,CAAC;SAChC,MAAM;YACL,IAAI,CAAC,cAAc,CAAC,KAAK,CAAC,CAAC;SAC5B;QAED,qBAAqB;uNACrB,UAAA,AAAO,EAAC,KAAK,CAAC,CAAC;QAEf,IAAI,UAAU,EAAE;YACd,OAAO,KAAK,CAAC;SACd,MAAM;YACL,KAAK,CAAC,OAAO,EAAE,CAAC;YAChB,OAAO,IAAI,CAAC;SACb;IACH,CAAC;IAED;;OAEG,CACH,IAAI,UAAU,GAAA;QACZ,IAAI,IAAI,CAAC,WAAW,IAAI,IAAI,EAAE;YAC5B,IAAI,CAAC,WAAW,GAAG,CAAC,CAAC;SACtB;QACD,OAAO,IAAI,CAAC,WAAW,CAAC;IAC1B,CAAC;IAES,mBAAmB,GAAA;QAC3B,IAAI,CAAC,WAAW,GAAG,IAAI,CAAC,UAAU,GAAG,CAAC,CAAC;IACzC,CAAC;IAED;;;;;;;;;;;;OAYG,CACH,gBAAgB,CAAC,CAAe,EAAE,OAAoB,EAAA;QAEpD,wNAAO,gBAAA,AAAa,EAAC,CAAC,EAAE,OAAO,CAAC,CAAC;IACnC,CAAC;IAYD;;OAEG,CACH,OAAO,GAAA;QACL,IAAI,IAAI,CAAC,WAAW,IAAI,IAAI,EAAE;YAC5B,yNAAA,AAAO,EAAC,IAAI,CAAC,WAAW,CAAC,CAAC;SAC3B;IACH,CAAC;IAED,KAAK,CAAC,cAAc,GAAA;QAClB,IAAI,IAAI,CAAC,WAAW,IAAI,IAAI,EAAE;YAC5B,IAAI,CAAC,WAAW,GAAG,CAAC,CAAC;SACtB;QACD,OAAO;YACL,IAAI,EAAE,MAAM;YACZ,+CAA+C;YAC/C,MAAM,uNAAE,SAAA,AAAM,EAAC,IAAI,CAAC,WAAW,EAAE,OAAO,CAAC;SAC1C,CAAC;IACJ,CAAC;IAED,KAAK,CAAC,UAAU,GAAA;QACd,MAAM,IAAI,KAAK,CAAC,yDAAyD,CAAC,CAAC;IAC7E,CAAC;IAED,KAAK,CAAC,UAAU,CAAC,YAA2B,EAAA;QAC1C,MAAM,IAAI,KAAK,CACX,CAAA,yDAAA,CAA2D,GAC3D,GAAG,IAAI,CAAC,YAAY,EAAE,EAAE,CAAC,CAAC;IAChC,CAAC;IAED;;;;;;OAMG,CACO,KAAK,CAAC,iBAAiB,CAAC,YAA2B,EAAA;QAE3D,IAAI,CAAC,WAAW,GAAG,CAAC,MAAM,YAAY,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QAC5D,OAAO,YAAY,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;IAC/B,CAAC;CACF;AAED,MAAM,CAAC,cAAc,CAAC,SAAS,EAAE,MAAM,CAAC,WAAW,EAAE;IACnD,KAAK,EAAE,CAAC,QAAmB,EAAE,EAAE;QAC7B,OAAO,QAAQ,CAAC,QAAQ,IAAI,IAAI,IAAI,QAAQ,CAAC,gBAAgB,IAAI,IAAI,IACjE,QAAQ,CAAC,cAAc,IAAI,IAAI,CAAC;IACtC,CAAC;CACF,CAAC,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 138, "column": 0}, "map": {"version":3,"file":"adadelta_optimizer.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-core/src/optimizers/adadelta_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {mul} from '../ops/mul';\nimport {sqrt} from '../ops/ops';\nimport {square} from '../ops/square';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, Serializable, SerializableConstructor} from '../serialization';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\n/** @doclink Optimizer */\nexport class AdadeltaOptimizer extends Optimizer {\n  /** @nocollapse */\n  static get className() {\n    // Name matters for Python compatibility.\n    // This is a getter instead of a property because when it's a property, it\n    // prevents the entire class from being tree-shaken.\n    return 'Adadelta';\n  }\n  private accumulatedGrads: OptimizerVariable[] = [];\n  private accumulatedUpdates: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected rho: number,\n      protected epsilon: number = null) {\n    super();\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n      if (this.accumulatedGrads[i] == null) {\n        this.accumulatedGrads[i] = {\n          originalName: `${name}/accum_grad`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedUpdates[i] == null) {\n        this.accumulatedUpdates[i] = {\n          originalName: `${name}/accum_var`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedGrad = this.accumulatedGrads[i].variable;\n      const accumulatedUpdate = this.accumulatedUpdates[i].variable;\n\n      tidy(() => {\n        const newAccumulatedGrad =\n            add(mul(accumulatedGrad, this.rho),\n                mul(square(gradient), 1 - this.rho));\n\n        const updates =\n            mul(div(sqrt(add(accumulatedUpdate, this.epsilon)),\n                    sqrt(add(accumulatedGrad, this.epsilon))),\n                gradient);\n\n        const newAccumulatedUpdate =\n            add(mul(accumulatedUpdate, this.rho),\n                mul(square(updates), 1 - this.rho));\n\n        accumulatedGrad.assign(newAccumulatedGrad);\n        accumulatedUpdate.assign(newAccumulatedUpdate);\n\n        const newValue = add(mul(updates, -this.learningRate), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  override dispose(): void {\n    if (this.accumulatedUpdates != null) {\n      dispose(this.accumulatedGrads.map(v => v.variable));\n      dispose(this.accumulatedUpdates.map(v => v.variable));\n    }\n  }\n\n  override async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    const variables: OptimizerVariable[] =\n        [...this.accumulatedGrads, ...this.accumulatedUpdates];\n    return [await this.saveIterations()].concat(\n        variables.map(v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  override async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount = weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedGrads =\n        weightValues.slice(0, variableCount).map(v => ({\n                                                   originalName: v.name,\n                                                   variable: v.tensor.variable(\n                                                       trainable)\n                                                 }));\n    this.accumulatedUpdates =\n        weightValues.slice(variableCount, variableCount * 2)\n            .map(v => ({\n                   originalName: v.name,\n                   variable: v.tensor.variable(trainable)\n                 }));\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'rho': this.rho,\n      'epsilon': this.epsilon\n    };\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(config['learningRate'], config['rho'], config['epsilon']);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;;;AAEH,OAAO,EAAC,MAAM,EAAC,MAAM,WAAW,CAAC;AACjC,OAAO,EAAC,OAAO,EAAE,IAAI,EAAC,MAAM,YAAY,CAAC;AACzC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,IAAI,EAAC,MAAM,YAAY,CAAC;AAChC,OAAO,EAAC,MAAM,EAAC,MAAM,eAAe,CAAC;AACrC,OAAO,EAAC,SAAS,EAAC,MAAM,mBAAmB,CAAC;AAI5C,OAAO,EAAC,SAAS,EAAoB,MAAM,aAAa,CAAC;;;;;;;;;;AAGnD,MAAO,iBAAkB,oOAAQ,YAAS;IAC9C,gBAAA,EAAkB,CAClB,MAAM,KAAK,SAAS,GAAA;QAClB,yCAAyC;QACzC,0EAA0E;QAC1E,oDAAoD;QACpD,OAAO,UAAU,CAAC;IACpB,CAAC;IAID,YACc,YAAoB,EAAY,GAAW,EAC3C,UAAkB,IAAI,CAAA;QAClC,KAAK,EAAE,CAAC;QAFI,IAAA,CAAA,YAAY,GAAZ,YAAY,CAAQ;QAAY,IAAA,CAAA,GAAG,GAAH,GAAG,CAAQ;QAC3C,IAAA,CAAA,OAAO,GAAP,OAAO,CAAe;QAL5B,IAAA,CAAA,gBAAgB,GAAwB,EAAE,CAAC;QAC3C,IAAA,CAAA,kBAAkB,GAAwB,EAAE,CAAC;QAOnD,IAAI,OAAO,IAAI,IAAI,EAAE;YACnB,IAAI,CAAC,OAAO,6MAAG,SAAM,CAAC,OAAO,CAAC,OAAO,EAAE,CAAC;SACzC;IACH,CAAC;IAED,cAAc,CAAC,iBAAiD,EAAA;QAC9D,MAAM,aAAa,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CACpD,iBAAiB,CAAC,GAAG,EAAC,IAAI,CAAC,EAAE,AAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAC1C,MAAM,CAAC,IAAI,CAAC,iBAAiB,CAAC,CAAC;QAEnC,aAAa,CAAC,OAAO,CAAC,CAAC,IAAI,EAAE,CAAC,EAAE,EAAE;YAChC,MAAM,KAAK,6MAAG,SAAM,CAAC,mBAAmB,CAAC,IAAI,CAAC,CAAC;YAC/C,MAAM,SAAS,GAAG,KAAK,CAAC;YACxB,IAAI,IAAI,CAAC,gBAAgB,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;gBACpC,IAAI,CAAC,gBAAgB,CAAC,CAAC,CAAC,GAAG;oBACzB,YAAY,EAAE,GAAG,IAAI,CAAA,WAAA,CAAa;oBAClC,QAAQ,iNAAE,OAAA,AAAI,EAAC,GAAG,EAAE,wNAAC,YAAA,AAAS,EAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;iBAC3D,CAAC;aACH;YACD,IAAI,IAAI,CAAC,kBAAkB,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;gBACtC,IAAI,CAAC,kBAAkB,CAAC,CAAC,CAAC,GAAG;oBAC3B,YAAY,EAAE,GAAG,IAAI,CAAA,UAAA,CAAY;oBACjC,QAAQ,iNAAE,OAAA,AAAI,EAAC,GAAG,EAAE,wNAAC,YAAA,AAAS,EAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;iBAC3D,CAAC;aACH;YAED,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CAC/C,iBAAiB,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAC7B,iBAAiB,CAAC,IAAI,CAAC,CAAC;YAC5B,IAAI,QAAQ,IAAI,IAAI,EAAE;gBACpB,OAAO;aACR;YAED,MAAM,eAAe,GAAG,IAAI,CAAC,gBAAgB,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC;YAC1D,MAAM,iBAAiB,GAAG,IAAI,CAAC,kBAAkB,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC;gBAE9D,kNAAA,AAAI,EAAC,GAAG,EAAE;gBACR,MAAM,kBAAkB,qNACpB,MAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,eAAe,EAAE,IAAI,CAAC,GAAG,CAAC,oNAC9B,MAAA,AAAG,uNAAC,SAAA,AAAM,EAAC,QAAQ,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC;gBAE7C,MAAM,OAAO,qNACT,MAAA,AAAG,oNAAC,MAAA,AAAG,GAAC,yNAAA,AAAI,oNAAC,MAAA,AAAG,EAAC,iBAAiB,EAAE,IAAI,CAAC,OAAO,CAAC,CAAC,GAC1C,yNAAA,AAAI,oNAAC,MAAA,AAAG,EAAC,eAAe,EAAE,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,EAC7C,QAAQ,CAAC,CAAC;gBAElB,MAAM,oBAAoB,qNACtB,MAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,iBAAiB,EAAE,IAAI,CAAC,GAAG,CAAC,oNAChC,MAAA,AAAG,uNAAC,SAAA,AAAM,EAAC,OAAO,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC;gBAE5C,eAAe,CAAC,MAAM,CAAC,kBAAkB,CAAC,CAAC;gBAC3C,iBAAiB,CAAC,MAAM,CAAC,oBAAoB,CAAC,CAAC;gBAE/C,MAAM,QAAQ,qNAAG,MAAA,AAAG,MAAC,oNAAA,AAAG,EAAC,OAAO,EAAE,CAAC,IAAI,CAAC,YAAY,CAAC,EAAE,KAAK,CAAC,CAAC;gBAC9D,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;YACzB,CAAC,CAAC,CAAC;QACL,CAAC,CAAC,CAAC;QACH,IAAI,CAAC,mBAAmB,EAAE,CAAC;IAC7B,CAAC;IAEQ,OAAO,GAAA;QACd,IAAI,IAAI,CAAC,kBAAkB,IAAI,IAAI,EAAE;2NACnC,UAAA,AAAO,EAAC,IAAI,CAAC,gBAAgB,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;2NACpD,UAAA,AAAO,EAAC,IAAI,CAAC,kBAAkB,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;SACvD;IACH,CAAC;IAEQ,KAAK,CAAC,UAAU,GAAA;QACvB,0CAA0C;QAC1C,MAAM,SAAS,GACX,CAAC;eAAG,IAAI,CAAC,gBAAgB,EAAE;eAAG,IAAI,CAAC,kBAAkB;SAAC,CAAC;QAC3D,OAAO;YAAC,MAAM,IAAI,CAAC,cAAc,EAAE;SAAC,CAAC,MAAM,CACvC,SAAS,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC;gBAAC,IAAI,EAAE,CAAC,CAAC,YAAY;gBAAE,MAAM,EAAE,CAAC,CAAC,QAAQ;YAAA,CAAC,CAAC,CAAC,CAAC,CAAC;IACxE,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,YAA2B,EAAA;QACnD,YAAY,GAAG,MAAM,IAAI,CAAC,iBAAiB,CAAC,YAAY,CAAC,CAAC;QAC1D,MAAM,aAAa,GAAG,YAAY,CAAC,MAAM,GAAG,CAAC,CAAC;QAC9C,MAAM,SAAS,GAAG,KAAK,CAAC;QACxB,IAAI,CAAC,gBAAgB,GACjB,YAAY,CAAC,KAAK,CAAC,CAAC,EAAE,aAAa,CAAC,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC;gBACJ,YAAY,EAAE,CAAC,CAAC,IAAI;gBACpB,QAAQ,EAAE,CAAC,CAAC,MAAM,CAAC,QAAQ,CACvB,SAAS,CAAC;aACf,CAAC,CAAC,CAAC;QACjD,IAAI,CAAC,kBAAkB,GACnB,YAAY,CAAC,KAAK,CAAC,aAAa,EAAE,aAAa,GAAG,CAAC,CAAC,CAC/C,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC;gBACJ,YAAY,EAAE,CAAC,CAAC,IAAI;gBACpB,QAAQ,EAAE,CAAC,CAAC,MAAM,CAAC,QAAQ,CAAC,SAAS,CAAC;aACvC,CAAC,CAAC,CAAC;IACnB,CAAC;IAED,SAAS,GAAA;QACP,OAAO;YACL,cAAc,EAAE,IAAI,CAAC,YAAY;YACjC,KAAK,EAAE,IAAI,CAAC,GAAG;YACf,SAAS,EAAE,IAAI,CAAC,OAAO;SACxB,CAAC;IACJ,CAAC;IAED,gBAAA,EAAkB,CAClB,MAAM,CAAU,UAAU,CACtB,GAA+B,EAAE,MAAkB,EAAA;QACrD,OAAO,IAAI,GAAG,CAAC,MAAM,CAAC,cAAc,CAAC,EAAE,MAAM,CAAC,KAAK,CAAC,EAAE,MAAM,CAAC,SAAS,CAAC,CAAC,CAAC;IAC3E,CAAC;CACF","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 276, "column": 0}, "map": {"version":3,"file":"adagrad_optimizer.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-core/src/optimizers/adagrad_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {fill} from '../ops/fill';\nimport {mul} from '../ops/mul';\nimport {sqrt} from '../ops/sqrt';\nimport {square} from '../ops/square';\nimport {ConfigDict, Serializable, SerializableConstructor} from '../serialization';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\n/** @doclink Optimizer */\nexport class AdagradOptimizer extends Optimizer {\n  /** @nocollapse */\n  static get className() {\n    // Name matters for Python compatibility.\n    // This is a getter instead of a property because when it's a property, it\n    // prevents the entire class from being tree-shaken.\n    return 'Adagrad';\n  }\n\n  private accumulatedGrads: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, private initialAccumulatorValue = 0.1) {\n    super();\n  }\n\n  applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      if (this.accumulatedGrads[i] == null) {\n        const trainable = false;\n        this.accumulatedGrads[i] = {\n          originalName: `${name}/accumulator`,\n          variable: tidy(\n              () => fill(value.shape, this.initialAccumulatorValue)\n                        .variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedGrad = this.accumulatedGrads[i].variable;\n\n      tidy(() => {\n        const newAccumulatedGrad = add(accumulatedGrad, square(gradient));\n        accumulatedGrad.assign(newAccumulatedGrad);\n\n        const newValue = add(\n            mul(div(gradient,\n                    sqrt(add(newAccumulatedGrad, ENGINE.backend.epsilon()))),\n                -this.learningRate),\n            value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  override dispose(): void {\n    if (this.accumulatedGrads != null) {\n      dispose(this.accumulatedGrads.map(v => v.variable));\n    }\n  }\n\n  override async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    return [await this.saveIterations()].concat(this.accumulatedGrads.map(\n        v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  override async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const trainable = false;\n    this.accumulatedGrads = weightValues.map(\n        v => ({originalName: v.name, variable: v.tensor.variable(trainable)}));\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'initialAccumulatorValue': this.initialAccumulatorValue,\n    };\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(config['learningRate'], config['initialAccumulatorValue']);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;;;AAEH,OAAO,EAAC,MAAM,EAAC,MAAM,WAAW,CAAC;AACjC,OAAO,EAAC,OAAO,EAAE,IAAI,EAAC,MAAM,YAAY,CAAC;AACzC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,IAAI,EAAC,MAAM,aAAa,CAAC;AACjC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,IAAI,EAAC,MAAM,aAAa,CAAC;AACjC,OAAO,EAAC,MAAM,EAAC,MAAM,eAAe,CAAC;AAIrC,OAAO,EAAC,SAAS,EAAoB,MAAM,aAAa,CAAC;;;;;;;;;;AAGnD,MAAO,gBAAiB,oOAAQ,YAAS;IAC7C,gBAAA,EAAkB,CAClB,MAAM,KAAK,SAAS,GAAA;QAClB,yCAAyC;QACzC,0EAA0E;QAC1E,oDAAoD;QACpD,OAAO,SAAS,CAAC;IACnB,CAAC;IAID,YACc,YAAoB,EAAU,0BAA0B,GAAG,CAAA;QACvE,KAAK,EAAE,CAAC;QADI,IAAA,CAAA,YAAY,GAAZ,YAAY,CAAQ;QAAU,IAAA,CAAA,uBAAuB,GAAvB,uBAAuB,CAAM;QAHjE,IAAA,CAAA,gBAAgB,GAAwB,EAAE,CAAC;IAKnD,CAAC;IAED,cAAc,CAAC,iBAAiD,EAAA;QAC9D,MAAM,aAAa,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CACpD,iBAAiB,CAAC,GAAG,EAAC,IAAI,CAAC,EAAE,AAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAC1C,MAAM,CAAC,IAAI,CAAC,iBAAiB,CAAC,CAAC;QAEnC,aAAa,CAAC,OAAO,CAAC,CAAC,IAAI,EAAE,CAAC,EAAE,EAAE;YAChC,MAAM,KAAK,6MAAG,SAAM,CAAC,mBAAmB,CAAC,IAAI,CAAC,CAAC;YAC/C,IAAI,IAAI,CAAC,gBAAgB,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;gBACpC,MAAM,SAAS,GAAG,KAAK,CAAC;gBACxB,IAAI,CAAC,gBAAgB,CAAC,CAAC,CAAC,GAAG;oBACzB,YAAY,EAAE,GAAG,IAAI,CAAA,YAAA,CAAc;oBACnC,QAAQ,GAAE,qNAAA,AAAI,EACV,GAAG,EAAE,kNAAC,OAAA,AAAI,EAAC,KAAK,CAAC,KAAK,EAAE,IAAI,CAAC,uBAAuB,CAAC,CAC1C,QAAQ,CAAC,SAAS,CAAC,CAAC;iBACpC,CAAC;aACH;YAED,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CAC/C,iBAAiB,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAC7B,iBAAiB,CAAC,IAAI,CAAC,CAAC;YAC5B,IAAI,QAAQ,IAAI,IAAI,EAAE;gBACpB,OAAO;aACR;YAED,MAAM,eAAe,GAAG,IAAI,CAAC,gBAAgB,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC;YAE1D,sNAAA,AAAI,EAAC,GAAG,EAAE;gBACR,MAAM,kBAAkB,qNAAG,MAAA,AAAG,EAAC,eAAe,uNAAE,SAAA,AAAM,EAAC,QAAQ,CAAC,CAAC,CAAC;gBAClE,eAAe,CAAC,MAAM,CAAC,kBAAkB,CAAC,CAAC;gBAE3C,MAAM,QAAQ,qNAAG,MAAA,AAAG,oNAChB,MAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,QAAQ,EACR,0NAAA,AAAI,oNAAC,MAAA,AAAG,EAAC,kBAAkB,4MAAE,SAAM,CAAC,OAAO,CAAC,OAAO,EAAE,CAAC,CAAC,CAAC,EAC5D,CAAC,IAAI,CAAC,YAAY,CAAC,EACvB,KAAK,CAAC,CAAC;gBACX,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;YACzB,CAAC,CAAC,CAAC;QACL,CAAC,CAAC,CAAC;QACH,IAAI,CAAC,mBAAmB,EAAE,CAAC;IAC7B,CAAC;IAEQ,OAAO,GAAA;QACd,IAAI,IAAI,CAAC,gBAAgB,IAAI,IAAI,EAAE;aACjC,wNAAA,AAAO,EAAC,IAAI,CAAC,gBAAgB,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;SACrD;IACH,CAAC;IAEQ,KAAK,CAAC,UAAU,GAAA;QACvB,0CAA0C;QAC1C,OAAO;YAAC,MAAM,IAAI,CAAC,cAAc,EAAE;SAAC,CAAC,MAAM,CAAC,IAAI,CAAC,gBAAgB,CAAC,GAAG,EACjE,CAAC,CAAC,EAAE,AAAC,CAAC;gBAAC,IAAI,EAAE,CAAC,CAAC,YAAY;gBAAE,MAAM,EAAE,CAAC,CAAC,QAAQ;YAAA,CAAC,CAAC,CAAC,CAAC,CAAC;IAC1D,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,YAA2B,EAAA;QACnD,YAAY,GAAG,MAAM,IAAI,CAAC,iBAAiB,CAAC,YAAY,CAAC,CAAC;QAC1D,MAAM,SAAS,GAAG,KAAK,CAAC;QACxB,IAAI,CAAC,gBAAgB,GAAG,YAAY,CAAC,GAAG,EACpC,CAAC,CAAC,EAAE,AAAC,CAAC;gBAAC,YAAY,EAAE,CAAC,CAAC,IAAI;gBAAE,QAAQ,EAAE,CAAC,CAAC,MAAM,CAAC,QAAQ,CAAC,SAAS,CAAC;YAAA,CAAC,CAAC,CAAC,CAAC;IAC7E,CAAC;IAED,SAAS,GAAA;QACP,OAAO;YACL,cAAc,EAAE,IAAI,CAAC,YAAY;YACjC,yBAAyB,EAAE,IAAI,CAAC,uBAAuB;SACxD,CAAC;IACJ,CAAC;IAED,gBAAA,EAAkB,CAClB,MAAM,CAAU,UAAU,CACtB,GAA+B,EAAE,MAAkB,EAAA;QACrD,OAAO,IAAI,GAAG,CAAC,MAAM,CAAC,cAAc,CAAC,EAAE,MAAM,CAAC,yBAAyB,CAAC,CAAC,CAAC;IAC5E,CAAC;CACF","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 388, "column": 0}, "map": {"version":3,"file":"adam_optimizer.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-core/src/optimizers/adam_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {mul} from '../ops/mul';\nimport {pow} from '../ops/pow';\nimport {scalar} from '../ops/scalar';\nimport {sqrt} from '../ops/sqrt';\nimport {square} from '../ops/square';\nimport {sub} from '../ops/sub';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, Serializable, SerializableConstructor} from '../serialization';\nimport {Variable} from '../tensor';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\nexport class AdamOptimizer extends Optimizer {\n  /** @nocollapse */\n  static get className() {\n    // Name matters for Python compatibility.\n    // This is a getter instead of a property because when it's a property, it\n    // prevents the entire class from being tree-shaken.\n    return 'Adam';\n  }\n  private accBeta1: Variable;\n  private accBeta2: Variable;\n\n  private accumulatedFirstMoment: OptimizerVariable[] = [];\n  private accumulatedSecondMoment: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected beta1: number,\n      protected beta2: number, protected epsilon: number = null) {\n    super();\n    tidy(() => {\n      // accB* will be updated by batch.\n      this.accBeta1 = scalar(beta1).variable();\n      this.accBeta2 = scalar(beta2).variable();\n    });\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const varNames = Array.isArray(variableGradients) ?\n        variableGradients.map(v => v.name) :\n        Object.keys(variableGradients);\n    tidy(() => {\n      const oneMinusAccBeta1 = sub(1, this.accBeta1);\n      const oneMinusAccBeta2 = sub(1, this.accBeta2);\n\n      varNames.forEach((name, i) => {\n        const value = ENGINE.registeredVariables[name];\n        const trainable = false;\n        if (this.accumulatedFirstMoment[i] == null) {\n          this.accumulatedFirstMoment[i] = {\n            originalName: `${name}/m`,\n            variable: tidy(() => zerosLike(value).variable(trainable))\n          };\n        }\n        if (this.accumulatedSecondMoment[i] == null) {\n          this.accumulatedSecondMoment[i] = {\n            originalName: `${name}/v`,\n            variable: tidy(() => zerosLike(value).variable(trainable))\n          };\n        }\n\n        const gradient = Array.isArray(variableGradients) ?\n            variableGradients[i].tensor :\n            variableGradients[name];\n        if (gradient == null) {\n          return;\n        }\n\n        const firstMoment = this.accumulatedFirstMoment[i].variable;\n        const secondMoment = this.accumulatedSecondMoment[i].variable;\n\n        const newFirstMoment =\n            add(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));\n        const newSecondMoment =\n            add(mul(secondMoment, this.beta2),\n                mul(square(gradient), 1 - this.beta2));\n\n        const biasCorrectedFirstMoment = div(newFirstMoment, oneMinusAccBeta1);\n        const biasCorrectedSecondMoment =\n            div(newSecondMoment, oneMinusAccBeta2);\n\n        firstMoment.assign(newFirstMoment);\n        secondMoment.assign(newSecondMoment);\n\n        const newValue =\n            add(mul(div(biasCorrectedFirstMoment,\n                        add(sqrt(biasCorrectedSecondMoment), this.epsilon)),\n                    -this.learningRate),\n                value);\n        value.assign(newValue);\n      });\n\n      this.accBeta1.assign(mul(this.accBeta1, this.beta1));\n      this.accBeta2.assign(mul(this.accBeta2, this.beta2));\n    });\n    this.incrementIterations();\n  }\n\n  override dispose(): void {\n    this.accBeta1.dispose();\n    this.accBeta2.dispose();\n\n    if (this.accumulatedFirstMoment != null) {\n      dispose(this.accumulatedFirstMoment.map(v => v.variable));\n    }\n    if (this.accumulatedSecondMoment != null) {\n      dispose(this.accumulatedSecondMoment.map(v => v.variable));\n    }\n  }\n\n  override async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    const variables: OptimizerVariable[] =\n        [...this.accumulatedFirstMoment, ...this.accumulatedSecondMoment];\n    return [await this.saveIterations()].concat(\n        variables.map(v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  override async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    tidy(() => {\n      this.accBeta1.assign(pow(this.beta1, this.iterations_ + 1));\n      this.accBeta2.assign(pow(this.beta2, this.iterations_ + 1));\n    });\n\n    const variableCount = weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedFirstMoment =\n        weightValues.slice(0, variableCount).map(v => ({\n                                                   originalName: v.name,\n                                                   variable: v.tensor.variable(\n                                                       trainable)\n                                                 }));\n    this.accumulatedSecondMoment =\n        weightValues.slice(variableCount, variableCount * 2)\n            .map(v => ({\n                   originalName: v.name,\n                   variable: v.tensor.variable(trainable)\n                 }));\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'beta1': this.beta1,\n      'beta2': this.beta2,\n      'epsilon': this.epsilon,\n    };\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config['learningRate'], config['beta1'], config['beta2'],\n        config['epsilon']);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;;;AAEH,OAAO,EAAC,MAAM,EAAC,MAAM,WAAW,CAAC;AACjC,OAAO,EAAC,OAAO,EAAE,IAAI,EAAC,MAAM,YAAY,CAAC;AACzC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,MAAM,EAAC,MAAM,eAAe,CAAC;AACrC,OAAO,EAAC,IAAI,EAAC,MAAM,aAAa,CAAC;AACjC,OAAO,EAAC,MAAM,EAAC,MAAM,eAAe,CAAC;AACrC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,SAAS,EAAC,MAAM,mBAAmB,CAAC;AAK5C,OAAO,EAAC,SAAS,EAAoB,MAAM,aAAa,CAAC;;;;;;;;;;;;;AAEnD,MAAO,aAAc,oOAAQ,YAAS;IAC1C,gBAAA,EAAkB,CAClB,MAAM,KAAK,SAAS,GAAA;QAClB,yCAAyC;QACzC,0EAA0E;QAC1E,oDAAoD;QACpD,OAAO,MAAM,CAAC;IAChB,CAAC;IAOD,YACc,YAAoB,EAAY,KAAa,EAC7C,KAAa,EAAY,UAAkB,IAAI,CAAA;QAC3D,KAAK,EAAE,CAAC;QAFI,IAAA,CAAA,YAAY,GAAZ,YAAY,CAAQ;QAAY,IAAA,CAAA,KAAK,GAAL,KAAK,CAAQ;QAC7C,IAAA,CAAA,KAAK,GAAL,KAAK,CAAQ;QAAY,IAAA,CAAA,OAAO,GAAP,OAAO,CAAe;QALrD,IAAA,CAAA,sBAAsB,GAAwB,EAAE,CAAC;QACjD,IAAA,CAAA,uBAAuB,GAAwB,EAAE,CAAC;sNAMxD,QAAA,AAAI,EAAC,GAAG,EAAE;YACR,kCAAkC;YAClC,IAAI,CAAC,QAAQ,wNAAG,SAAA,AAAM,EAAC,KAAK,CAAC,CAAC,QAAQ,EAAE,CAAC;YACzC,IAAI,CAAC,QAAQ,wNAAG,SAAA,AAAM,EAAC,KAAK,CAAC,CAAC,QAAQ,EAAE,CAAC;QAC3C,CAAC,CAAC,CAAC;QAEH,IAAI,OAAO,IAAI,IAAI,EAAE;YACnB,IAAI,CAAC,OAAO,6MAAG,SAAM,CAAC,OAAO,CAAC,OAAO,EAAE,CAAC;SACzC;IACH,CAAC;IAED,cAAc,CAAC,iBAAiD,EAAA;QAC9D,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CAC/C,iBAAiB,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CACpC,MAAM,CAAC,IAAI,CAAC,iBAAiB,CAAC,CAAC;uNACnC,OAAA,AAAI,EAAC,GAAG,EAAE;YACR,MAAM,gBAAgB,qNAAG,MAAA,AAAG,EAAC,CAAC,EAAE,IAAI,CAAC,QAAQ,CAAC,CAAC;YAC/C,MAAM,gBAAgB,OAAG,oNAAA,AAAG,EAAC,CAAC,EAAE,IAAI,CAAC,QAAQ,CAAC,CAAC;YAE/C,QAAQ,CAAC,OAAO,CAAC,CAAC,IAAI,EAAE,CAAC,EAAE,EAAE;gBAC3B,MAAM,KAAK,6MAAG,SAAM,CAAC,mBAAmB,CAAC,IAAI,CAAC,CAAC;gBAC/C,MAAM,SAAS,GAAG,KAAK,CAAC;gBACxB,IAAI,IAAI,CAAC,sBAAsB,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;oBAC1C,IAAI,CAAC,sBAAsB,CAAC,CAAC,CAAC,GAAG;wBAC/B,YAAY,EAAE,GAAG,IAAI,CAAA,EAAA,CAAI;wBACzB,QAAQ,MAAE,kNAAA,AAAI,EAAC,GAAG,EAAE,wNAAC,YAAA,AAAS,EAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;qBAC3D,CAAC;iBACH;gBACD,IAAI,IAAI,CAAC,uBAAuB,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;oBAC3C,IAAI,CAAC,uBAAuB,CAAC,CAAC,CAAC,GAAG;wBAChC,YAAY,EAAE,GAAG,IAAI,CAAA,EAAA,CAAI;wBACzB,QAAQ,iNAAE,OAAA,AAAI,EAAC,GAAG,EAAE,wNAAC,YAAA,AAAS,EAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;qBAC3D,CAAC;iBACH;gBAED,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CAC/C,iBAAiB,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAC7B,iBAAiB,CAAC,IAAI,CAAC,CAAC;gBAC5B,IAAI,QAAQ,IAAI,IAAI,EAAE;oBACpB,OAAO;iBACR;gBAED,MAAM,WAAW,GAAG,IAAI,CAAC,sBAAsB,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC;gBAC5D,MAAM,YAAY,GAAG,IAAI,CAAC,uBAAuB,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC;gBAE9D,MAAM,cAAc,qNAChB,MAAA,AAAG,MAAC,oNAAA,AAAG,EAAC,WAAW,EAAE,IAAI,CAAC,KAAK,CAAC,oNAAE,MAAA,AAAG,EAAC,QAAQ,EAAE,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC;gBACrE,MAAM,eAAe,IACjB,uNAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,YAAY,EAAE,IAAI,CAAC,KAAK,CAAC,oNAC7B,MAAA,AAAG,MAAC,0NAAA,AAAM,EAAC,QAAQ,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC;gBAE/C,MAAM,wBAAwB,qNAAG,MAAA,AAAG,EAAC,cAAc,EAAE,gBAAgB,CAAC,CAAC;gBACvE,MAAM,yBAAyB,IAC3B,uNAAA,AAAG,EAAC,eAAe,EAAE,gBAAgB,CAAC,CAAC;gBAE3C,WAAW,CAAC,MAAM,CAAC,cAAc,CAAC,CAAC;gBACnC,YAAY,CAAC,MAAM,CAAC,eAAe,CAAC,CAAC;gBAErC,MAAM,QAAQ,IACV,uNAAA,AAAG,oNAAC,MAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,wBAAwB,oNACxB,MAAA,AAAG,GAAC,yNAAA,AAAI,EAAC,yBAAyB,CAAC,EAAE,IAAI,CAAC,OAAO,CAAC,CAAC,EACvD,CAAC,IAAI,CAAC,YAAY,CAAC,EACvB,KAAK,CAAC,CAAC;gBACf,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;YACzB,CAAC,CAAC,CAAC;YAEH,IAAI,CAAC,QAAQ,CAAC,MAAM,CAAC,wNAAA,AAAG,EAAC,IAAI,CAAC,QAAQ,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC;YACrD,IAAI,CAAC,QAAQ,CAAC,MAAM,KAAC,oNAAA,AAAG,EAAC,IAAI,CAAC,QAAQ,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC;QACvD,CAAC,CAAC,CAAC;QACH,IAAI,CAAC,mBAAmB,EAAE,CAAC;IAC7B,CAAC;IAEQ,OAAO,GAAA;QACd,IAAI,CAAC,QAAQ,CAAC,OAAO,EAAE,CAAC;QACxB,IAAI,CAAC,QAAQ,CAAC,OAAO,EAAE,CAAC;QAExB,IAAI,IAAI,CAAC,sBAAsB,IAAI,IAAI,EAAE;YACvC,yNAAA,AAAO,EAAC,IAAI,CAAC,sBAAsB,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;SAC3D;QACD,IAAI,IAAI,CAAC,uBAAuB,IAAI,IAAI,EAAE;2NACxC,UAAA,AAAO,EAAC,IAAI,CAAC,uBAAuB,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;SAC5D;IACH,CAAC;IAEQ,KAAK,CAAC,UAAU,GAAA;QACvB,0CAA0C;QAC1C,MAAM,SAAS,GACX,CAAC;eAAG,IAAI,CAAC,sBAAsB,EAAE;eAAG,IAAI,CAAC,uBAAuB;SAAC,CAAC;QACtE,OAAO;YAAC,MAAM,IAAI,CAAC,cAAc,EAAE;SAAC,CAAC,MAAM,CACvC,SAAS,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC;gBAAC,IAAI,EAAE,CAAC,CAAC,YAAY;gBAAE,MAAM,EAAE,CAAC,CAAC,QAAQ;YAAA,CAAC,CAAC,CAAC,CAAC,CAAC;IACxE,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,YAA2B,EAAA;QACnD,YAAY,GAAG,MAAM,IAAI,CAAC,iBAAiB,CAAC,YAAY,CAAC,CAAC;uNAC1D,OAAA,AAAI,EAAC,GAAG,EAAE;YACR,IAAI,CAAC,QAAQ,CAAC,MAAM,mNAAC,MAAA,AAAG,EAAC,IAAI,CAAC,KAAK,EAAE,IAAI,CAAC,WAAW,GAAG,CAAC,CAAC,CAAC,CAAC;YAC5D,IAAI,CAAC,QAAQ,CAAC,MAAM,KAAC,oNAAA,AAAG,EAAC,IAAI,CAAC,KAAK,EAAE,IAAI,CAAC,WAAW,GAAG,CAAC,CAAC,CAAC,CAAC;QAC9D,CAAC,CAAC,CAAC;QAEH,MAAM,aAAa,GAAG,YAAY,CAAC,MAAM,GAAG,CAAC,CAAC;QAC9C,MAAM,SAAS,GAAG,KAAK,CAAC;QACxB,IAAI,CAAC,sBAAsB,GACvB,YAAY,CAAC,KAAK,CAAC,CAAC,EAAE,aAAa,CAAC,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC;gBACJ,YAAY,EAAE,CAAC,CAAC,IAAI;gBACpB,QAAQ,EAAE,CAAC,CAAC,MAAM,CAAC,QAAQ,CACvB,SAAS,CAAC;aACf,CAAC,CAAC,CAAC;QACjD,IAAI,CAAC,uBAAuB,GACxB,YAAY,CAAC,KAAK,CAAC,aAAa,EAAE,aAAa,GAAG,CAAC,CAAC,CAC/C,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC;gBACJ,YAAY,EAAE,CAAC,CAAC,IAAI;gBACpB,QAAQ,EAAE,CAAC,CAAC,MAAM,CAAC,QAAQ,CAAC,SAAS,CAAC;aACvC,CAAC,CAAC,CAAC;IACnB,CAAC;IAED,SAAS,GAAA;QACP,OAAO;YACL,cAAc,EAAE,IAAI,CAAC,YAAY;YACjC,OAAO,EAAE,IAAI,CAAC,KAAK;YACnB,OAAO,EAAE,IAAI,CAAC,KAAK;YACnB,SAAS,EAAE,IAAI,CAAC,OAAO;SACxB,CAAC;IACJ,CAAC;IAED,gBAAA,EAAkB,CAClB,MAAM,CAAU,UAAU,CACtB,GAA+B,EAAE,MAAkB,EAAA;QACrD,OAAO,IAAI,GAAG,CACV,MAAM,CAAC,cAAc,CAAC,EAAE,MAAM,CAAC,OAAO,CAAC,EAAE,MAAM,CAAC,OAAO,CAAC,EACxD,MAAM,CAAC,SAAS,CAAC,CAAC,CAAC;IACzB,CAAC;CACF","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 552, "column": 0}, "map": {"version":3,"file":"adamax_optimizer.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-core/src/optimizers/adamax_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {abs} from '../ops/abs';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {maximum} from '../ops/maximum';\nimport {mul} from '../ops/mul';\nimport {scalar} from '../ops/scalar';\nimport {sub} from '../ops/sub';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, Serializable, SerializableConstructor} from '../serialization';\nimport {Variable} from '../tensor';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\nexport class AdamaxOptimizer extends Optimizer {\n  /** @nocollapse */\n  static get className() {\n    // Name matters for Python compatibility.\n    // This is a getter instead of a property because when it's a property, it\n    // prevents the entire class from being tree-shaken.\n    return 'Adamax';\n  }\n  private accBeta1: Variable;\n  private iteration: Variable;\n\n  private accumulatedFirstMoment: OptimizerVariable[] = [];\n  private accumulatedWeightedInfNorm: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected beta1: number,\n      protected beta2: number, protected epsilon: number = null,\n      protected decay = 0.0) {\n    super();\n\n    tidy(() => {\n      this.iteration = scalar(0).variable();\n      this.accBeta1 = scalar(beta1).variable();\n    });\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    tidy(() => {\n      const oneMinusAccBeta1 = sub(1, this.accBeta1);\n      const lr =\n          div(-this.learningRate, add(mul(this.iteration, this.decay), 1));\n\n      variableNames.forEach((name, i) => {\n        const value = ENGINE.registeredVariables[name];\n        const trainable = false;\n        if (this.accumulatedFirstMoment[i] == null) {\n          this.accumulatedFirstMoment[i] = {\n            originalName: `${name}/m`,\n            variable: zerosLike(value).variable(trainable)\n          };\n        }\n        if (this.accumulatedWeightedInfNorm[i] == null) {\n          this.accumulatedWeightedInfNorm[i] = {\n            originalName: `${name}/v`,\n            variable: zerosLike(value).variable(trainable)\n          };\n        }\n\n        const gradient = Array.isArray(variableGradients) ?\n            variableGradients[i].tensor :\n            variableGradients[name];\n        if (gradient == null) {\n          return;\n        }\n\n        const firstMoment = this.accumulatedFirstMoment[i].variable;\n        const weightedInfNorm = this.accumulatedWeightedInfNorm[i].variable;\n\n        const newFirstMoment =\n            add(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));\n\n        const ut0 = mul(weightedInfNorm, this.beta2);\n        const ut1 = abs(gradient);\n\n        const newWeightedInfNorm = maximum(ut0, ut1);\n\n        firstMoment.assign(newFirstMoment);\n        weightedInfNorm.assign(newWeightedInfNorm);\n\n        const newValue =\n            add(mul(div(lr, oneMinusAccBeta1),\n                    div(newFirstMoment, add(newWeightedInfNorm, this.epsilon))),\n                value);\n\n        value.assign(newValue);\n      });\n\n      this.iteration.assign(add(this.iteration, 1));\n      this.accBeta1.assign(mul(this.accBeta1, this.beta1));\n    });\n    this.incrementIterations();\n  }\n\n  override dispose(): void {\n    this.accBeta1.dispose();\n    this.iteration.dispose();\n\n    if (this.accumulatedFirstMoment != null) {\n      dispose(this.accumulatedFirstMoment.map(v => v.variable));\n    }\n    if (this.accumulatedWeightedInfNorm != null) {\n      dispose(this.accumulatedWeightedInfNorm.map(v => v.variable));\n    }\n  }\n\n  override async getWeights(): Promise<NamedTensor[]> {\n    throw new Error('getWeights() is not implemented for Adamax yet.');\n  }\n\n  override async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    throw new Error('setWeights() is not implemented for Adamax yet.');\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'beta1': this.beta1,\n      'beta2': this.beta2,\n      'epsilon': this.epsilon,\n      'decay': this.decay\n    };\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config['learningRate'], config['beta1'], config['beta2'],\n        config['epsilon'], config['decay']);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;;;AAEH,OAAO,EAAC,MAAM,EAAC,MAAM,WAAW,CAAC;AACjC,OAAO,EAAC,OAAO,EAAE,IAAI,EAAC,MAAM,YAAY,CAAC;AACzC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,OAAO,EAAC,MAAM,gBAAgB,CAAC;AACvC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,MAAM,EAAC,MAAM,eAAe,CAAC;AACrC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,SAAS,EAAC,MAAM,mBAAmB,CAAC;AAK5C,OAAO,EAAC,SAAS,EAAoB,MAAM,aAAa,CAAC;;;;;;;;;;;;AAEnD,MAAO,eAAgB,oOAAQ,YAAS;IAC5C,gBAAA,EAAkB,CAClB,MAAM,KAAK,SAAS,GAAA;QAClB,yCAAyC;QACzC,0EAA0E;QAC1E,oDAAoD;QACpD,OAAO,QAAQ,CAAC;IAClB,CAAC;IAOD,YACc,YAAoB,EAAY,KAAa,EAC7C,KAAa,EAAY,UAAkB,IAAI,EAC/C,QAAQ,GAAG,CAAA;QACvB,KAAK,EAAE,CAAC;QAHI,IAAA,CAAA,YAAY,GAAZ,YAAY,CAAQ;QAAY,IAAA,CAAA,KAAK,GAAL,KAAK,CAAQ;QAC7C,IAAA,CAAA,KAAK,GAAL,KAAK,CAAQ;QAAY,IAAA,CAAA,OAAO,GAAP,OAAO,CAAe;QAC/C,IAAA,CAAA,KAAK,GAAL,KAAK,CAAM;QANjB,IAAA,CAAA,sBAAsB,GAAwB,EAAE,CAAC;QACjD,IAAA,CAAA,0BAA0B,GAAwB,EAAE,CAAC;uNAQ3D,OAAA,AAAI,EAAC,GAAG,EAAE;YACR,IAAI,CAAC,SAAS,GAAG,8NAAA,AAAM,EAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;YACtC,IAAI,CAAC,QAAQ,wNAAG,SAAA,AAAM,EAAC,KAAK,CAAC,CAAC,QAAQ,EAAE,CAAC;QAC3C,CAAC,CAAC,CAAC;QAEH,IAAI,OAAO,IAAI,IAAI,EAAE;YACnB,IAAI,CAAC,OAAO,6MAAG,SAAM,CAAC,OAAO,CAAC,OAAO,EAAE,CAAC;SACzC;IACH,CAAC;IAED,cAAc,CAAC,iBAAiD,EAAA;QAC9D,MAAM,aAAa,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CACpD,iBAAiB,CAAC,GAAG,EAAC,IAAI,CAAC,EAAE,AAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAC1C,MAAM,CAAC,IAAI,CAAC,iBAAiB,CAAC,CAAC;uNAEnC,OAAA,AAAI,EAAC,GAAG,EAAE;YACR,MAAM,gBAAgB,OAAG,oNAAA,AAAG,EAAC,CAAC,EAAE,IAAI,CAAC,QAAQ,CAAC,CAAC;YAC/C,MAAM,EAAE,qNACJ,MAAA,AAAG,EAAC,CAAC,IAAI,CAAC,YAAY,oNAAE,MAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,IAAI,CAAC,SAAS,EAAE,IAAI,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YAErE,aAAa,CAAC,OAAO,CAAC,CAAC,IAAI,EAAE,CAAC,EAAE,EAAE;gBAChC,MAAM,KAAK,6MAAG,SAAM,CAAC,mBAAmB,CAAC,IAAI,CAAC,CAAC;gBAC/C,MAAM,SAAS,GAAG,KAAK,CAAC;gBACxB,IAAI,IAAI,CAAC,sBAAsB,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;oBAC1C,IAAI,CAAC,sBAAsB,CAAC,CAAC,CAAC,GAAG;wBAC/B,YAAY,EAAE,GAAG,IAAI,CAAA,EAAA,CAAI;wBACzB,QAAQ,MAAE,iOAAA,AAAS,EAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC;qBAC/C,CAAC;iBACH;gBACD,IAAI,IAAI,CAAC,0BAA0B,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;oBAC9C,IAAI,CAAC,0BAA0B,CAAC,CAAC,CAAC,GAAG;wBACnC,YAAY,EAAE,GAAG,IAAI,CAAA,EAAA,CAAI;wBACzB,QAAQ,2NAAE,YAAA,AAAS,EAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC;qBAC/C,CAAC;iBACH;gBAED,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CAC/C,iBAAiB,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAC7B,iBAAiB,CAAC,IAAI,CAAC,CAAC;gBAC5B,IAAI,QAAQ,IAAI,IAAI,EAAE;oBACpB,OAAO;iBACR;gBAED,MAAM,WAAW,GAAG,IAAI,CAAC,sBAAsB,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC;gBAC5D,MAAM,eAAe,GAAG,IAAI,CAAC,0BAA0B,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC;gBAEpE,MAAM,cAAc,qNAChB,MAAA,AAAG,GAAC,uNAAA,AAAG,EAAC,WAAW,EAAE,IAAI,CAAC,KAAK,CAAC,oNAAE,MAAA,AAAG,EAAC,QAAQ,EAAE,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC;gBAErE,MAAM,GAAG,qNAAG,MAAA,AAAG,EAAC,eAAe,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC;gBAC7C,MAAM,GAAG,qNAAG,MAAA,AAAG,EAAC,QAAQ,CAAC,CAAC;gBAE1B,MAAM,kBAAkB,GAAG,gOAAA,AAAO,EAAC,GAAG,EAAE,GAAG,CAAC,CAAC;gBAE7C,WAAW,CAAC,MAAM,CAAC,cAAc,CAAC,CAAC;gBACnC,eAAe,CAAC,MAAM,CAAC,kBAAkB,CAAC,CAAC;gBAE3C,MAAM,QAAQ,OACV,oNAAA,AAAG,oNAAC,MAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,EAAE,EAAE,gBAAgB,CAAC,oNACzB,MAAA,AAAG,EAAC,cAAc,oNAAE,MAAA,AAAG,EAAC,kBAAkB,EAAE,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,EAC/D,KAAK,CAAC,CAAC;gBAEf,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;YACzB,CAAC,CAAC,CAAC;YAEH,IAAI,CAAC,SAAS,CAAC,MAAM,KAAC,oNAAA,AAAG,EAAC,IAAI,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC;YAC9C,IAAI,CAAC,QAAQ,CAAC,MAAM,mNAAC,MAAA,AAAG,EAAC,IAAI,CAAC,QAAQ,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC;QACvD,CAAC,CAAC,CAAC;QACH,IAAI,CAAC,mBAAmB,EAAE,CAAC;IAC7B,CAAC;IAEQ,OAAO,GAAA;QACd,IAAI,CAAC,QAAQ,CAAC,OAAO,EAAE,CAAC;QACxB,IAAI,CAAC,SAAS,CAAC,OAAO,EAAE,CAAC;QAEzB,IAAI,IAAI,CAAC,sBAAsB,IAAI,IAAI,EAAE;2NACvC,UAAA,AAAO,EAAC,IAAI,CAAC,sBAAsB,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;SAC3D;QACD,IAAI,IAAI,CAAC,0BAA0B,IAAI,IAAI,EAAE;2NAC3C,UAAA,AAAO,EAAC,IAAI,CAAC,0BAA0B,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;SAC/D;IACH,CAAC;IAEQ,KAAK,CAAC,UAAU,GAAA;QACvB,MAAM,IAAI,KAAK,CAAC,iDAAiD,CAAC,CAAC;IACrE,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,YAA2B,EAAA;QACnD,MAAM,IAAI,KAAK,CAAC,iDAAiD,CAAC,CAAC;IACrE,CAAC;IAED,SAAS,GAAA;QACP,OAAO;YACL,cAAc,EAAE,IAAI,CAAC,YAAY;YACjC,OAAO,EAAE,IAAI,CAAC,KAAK;YACnB,OAAO,EAAE,IAAI,CAAC,KAAK;YACnB,SAAS,EAAE,IAAI,CAAC,OAAO;YACvB,OAAO,EAAE,IAAI,CAAC,KAAK;SACpB,CAAC;IACJ,CAAC;IAED,gBAAA,EAAkB,CAClB,MAAM,CAAU,UAAU,CACtB,GAA+B,EAAE,MAAkB,EAAA;QACrD,OAAO,IAAI,GAAG,CACV,MAAM,CAAC,cAAc,CAAC,EAAE,MAAM,CAAC,OAAO,CAAC,EAAE,MAAM,CAAC,OAAO,CAAC,EACxD,MAAM,CAAC,SAAS,CAAC,EAAE,MAAM,CAAC,OAAO,CAAC,CAAC,CAAC;IAC1C,CAAC;CACF","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 691, "column": 0}, "map": {"version":3,"file":"sgd_optimizer.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-core/src/optimizers/sgd_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {keep, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {mul} from '../ops/mul';\nimport {scalar} from '../ops/scalar';\nimport {ConfigDict, Serializable, SerializableConstructor} from '../serialization';\nimport {Scalar} from '../tensor';\nimport {NamedTensor, NamedTensorMap} from '../tensor_types';\n\nimport {Optimizer} from './optimizer';\n\n/** @doclink Optimizer */\nexport class SGDOptimizer extends Optimizer {\n  /** @nocollapse */\n  static get className() {\n    // Name matters for Python compatibility.\n    // This is a getter instead of a property because when it's a property, it\n    // prevents the entire class from being tree-shaken.\n    return 'SGD';\n  }\n  protected c: Scalar;\n\n  constructor(protected learningRate: number) {\n    super();\n    this.setLearningRate(learningRate);\n  }\n\n  applyGradients(variableGradients: NamedTensorMap|NamedTensor[]) {\n    const varNames = Array.isArray(variableGradients) ?\n        variableGradients.map(v => v.name) :\n        Object.keys(variableGradients);\n    varNames.forEach((name, i) => {\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n      const value = ENGINE.registeredVariables[name];\n      tidy(() => {\n        const newValue = add(mul(this.c, gradient), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  /**\n   * Sets the learning rate of the optimizer.\n   */\n  setLearningRate(learningRate: number) {\n    this.learningRate = learningRate;\n    if (this.c != null) {\n      this.c.dispose();\n    }\n    this.c = keep(scalar(-learningRate));\n  }\n\n  override dispose() {\n    this.c.dispose();\n  }\n\n  override async getWeights(): Promise<NamedTensor[]> {\n    return [await this.saveIterations()];\n  }\n\n  override async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    if (weightValues.length !== 0) {\n      throw new Error('SGD optimizer does not have settable weights.');\n    }\n  }\n\n  getConfig(): ConfigDict {\n    return {'learningRate': this.learningRate};\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(config['learningRate']);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;;;AAEH,OAAO,EAAC,MAAM,EAAC,MAAM,WAAW,CAAC;AACjC,OAAO,EAAC,IAAI,EAAE,IAAI,EAAC,MAAM,YAAY,CAAC;AACtC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,MAAM,EAAC,MAAM,eAAe,CAAC;AAKrC,OAAO,EAAC,SAAS,EAAC,MAAM,aAAa,CAAC;;;;;;;AAGhC,MAAO,YAAa,oOAAQ,YAAS;IACzC,gBAAA,EAAkB,CAClB,MAAM,KAAK,SAAS,GAAA;QAClB,yCAAyC;QACzC,0EAA0E;QAC1E,oDAAoD;QACpD,OAAO,KAAK,CAAC;IACf,CAAC;IAGD,YAAsB,YAAoB,CAAA;QACxC,KAAK,EAAE,CAAC;QADY,IAAA,CAAA,YAAY,GAAZ,YAAY,CAAQ;QAExC,IAAI,CAAC,eAAe,CAAC,YAAY,CAAC,CAAC;IACrC,CAAC;IAED,cAAc,CAAC,iBAA+C,EAAA;QAC5D,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CAC/C,iBAAiB,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CACpC,MAAM,CAAC,IAAI,CAAC,iBAAiB,CAAC,CAAC;QACnC,QAAQ,CAAC,OAAO,CAAC,CAAC,IAAI,EAAE,CAAC,EAAE,EAAE;YAC3B,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CAC/C,iBAAiB,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAC7B,iBAAiB,CAAC,IAAI,CAAC,CAAC;YAC5B,IAAI,QAAQ,IAAI,IAAI,EAAE;gBACpB,OAAO;aACR;YACD,MAAM,KAAK,6MAAG,SAAM,CAAC,mBAAmB,CAAC,IAAI,CAAC,CAAC;2NAC/C,OAAA,AAAI,EAAC,GAAG,EAAE;gBACR,MAAM,QAAQ,IAAG,uNAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,IAAI,CAAC,CAAC,EAAE,QAAQ,CAAC,EAAE,KAAK,CAAC,CAAC;gBACnD,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;YACzB,CAAC,CAAC,CAAC;QACL,CAAC,CAAC,CAAC;QACH,IAAI,CAAC,mBAAmB,EAAE,CAAC;IAC7B,CAAC;IAED;;OAEG,CACH,eAAe,CAAC,YAAoB,EAAA;QAClC,IAAI,CAAC,YAAY,GAAG,YAAY,CAAC;QACjC,IAAI,IAAI,CAAC,CAAC,IAAI,IAAI,EAAE;YAClB,IAAI,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC;SAClB;QACD,IAAI,CAAC,CAAC,IAAG,qNAAA,AAAI,uNAAC,SAAA,AAAM,EAAC,CAAC,YAAY,CAAC,CAAC,CAAC;IACvC,CAAC;IAEQ,OAAO,GAAA;QACd,IAAI,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC;IACnB,CAAC;IAEQ,KAAK,CAAC,UAAU,GAAA;QACvB,OAAO;YAAC,MAAM,IAAI,CAAC,cAAc,EAAE;SAAC,CAAC;IACvC,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,YAA2B,EAAA;QACnD,YAAY,GAAG,MAAM,IAAI,CAAC,iBAAiB,CAAC,YAAY,CAAC,CAAC;QAC1D,IAAI,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;YAC7B,MAAM,IAAI,KAAK,CAAC,+CAA+C,CAAC,CAAC;SAClE;IACH,CAAC;IAED,SAAS,GAAA;QACP,OAAO;YAAC,cAAc,EAAE,IAAI,CAAC,YAAY;QAAA,CAAC,CAAC;IAC7C,CAAC;IAED,gBAAA,EAAkB,CAClB,MAAM,CAAU,UAAU,CACtB,GAA+B,EAAE,MAAkB,EAAA;QACrD,OAAO,IAAI,GAAG,CAAC,MAAM,CAAC,cAAc,CAAC,CAAC,CAAC;IACzC,CAAC;CACF","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 786, "column": 0}, "map": {"version":3,"file":"momentum_optimizer.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-core/src/optimizers/momentum_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {mul} from '../ops/mul';\nimport {scalar} from '../ops/scalar';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, Serializable, SerializableConstructor} from '../serialization';\nimport {Scalar, Tensor} from '../tensor';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {OptimizerVariable} from './optimizer';\nimport {SGDOptimizer} from './sgd_optimizer';\n\n/** @doclink Optimizer */\nexport class MomentumOptimizer extends SGDOptimizer {\n  /** @nocollapse */\n  // Name matters for Python compatibility.\n  static override get className() {\n    // Name matters for Python compatibility.\n    // This is a getter instead of a property because when it's a property, it\n    // prevents the entire class from being tree-shaken.\n    return 'Momentum';\n  }\n  private m: Scalar;\n  private accumulations: OptimizerVariable[] = [];\n\n  constructor(\n      protected override learningRate: number, private momentum: number,\n      private useNesterov = false) {\n    super(learningRate);\n    this.m = scalar(this.momentum);\n  }\n\n  override applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      if (this.accumulations[i] == null) {\n        const trainable = false;\n        this.accumulations[i] = {\n          originalName: `${name}/momentum`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const accumulation = this.accumulations[i].variable;\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      tidy(() => {\n        let newValue: Tensor;\n        const newAccumulation = add(mul(this.m, accumulation), gradient);\n        if (this.useNesterov) {\n          newValue = add(\n              mul(this.c, add(gradient, mul(newAccumulation, this.m))), value);\n        } else {\n          newValue = add(mul(this.c, newAccumulation), value);\n        }\n        accumulation.assign(newAccumulation);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  override dispose(): void {\n    this.m.dispose();\n    if (this.accumulations != null) {\n      dispose(this.accumulations.map(v => v.variable));\n    }\n  }\n\n  /**\n   * Sets the momentum of the optimizer.\n   *\n   * @param momentum\n   */\n  setMomentum(momentum: number) {\n    this.momentum = momentum;\n  }\n\n  override async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    return [await this.saveIterations()].concat(this.accumulations.map(\n        v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  override async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const trainable = false;\n    this.accumulations = weightValues.map(\n        v => ({originalName: v.name, variable: v.tensor.variable(trainable)}));\n  }\n\n  override getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'momentum': this.momentum,\n      'useNesterov': this.useNesterov\n    };\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config['learningRate'], config['momentum'], config['useNesterov']);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;;;AAEH,OAAO,EAAC,MAAM,EAAC,MAAM,WAAW,CAAC;AACjC,OAAO,EAAC,OAAO,EAAE,IAAI,EAAC,MAAM,YAAY,CAAC;AACzC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,MAAM,EAAC,MAAM,eAAe,CAAC;AACrC,OAAO,EAAC,SAAS,EAAC,MAAM,mBAAmB,CAAC;AAM5C,OAAO,EAAC,YAAY,EAAC,MAAM,iBAAiB,CAAC;;;;;;;;AAGvC,MAAO,iBAAkB,wOAAQ,eAAY;IACjD,gBAAA,EAAkB,CAClB,yCAAyC;IACzC,MAAM,KAAc,SAAS,GAAA;QAC3B,yCAAyC;QACzC,0EAA0E;QAC1E,oDAAoD;QACpD,OAAO,UAAU,CAAC;IACpB,CAAC;IAID,YACuB,YAAoB,EAAU,QAAgB,EACzD,cAAc,KAAK,CAAA;QAC7B,KAAK,CAAC,YAAY,CAAC,CAAC;QAFC,IAAA,CAAA,YAAY,GAAZ,YAAY,CAAQ;QAAU,IAAA,CAAA,QAAQ,GAAR,QAAQ,CAAQ;QACzD,IAAA,CAAA,WAAW,GAAX,WAAW,CAAQ;QAJvB,IAAA,CAAA,aAAa,GAAwB,EAAE,CAAC;QAM9C,IAAI,CAAC,CAAC,OAAG,0NAAA,AAAM,EAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;IACjC,CAAC;IAEQ,cAAc,CAAC,iBAAiD,EAAA;QACvE,MAAM,aAAa,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CACpD,iBAAiB,CAAC,GAAG,EAAC,IAAI,CAAC,EAAE,AAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAC1C,MAAM,CAAC,IAAI,CAAC,iBAAiB,CAAC,CAAC;QAEnC,aAAa,CAAC,OAAO,CAAC,CAAC,IAAI,EAAE,CAAC,EAAE,EAAE;YAChC,MAAM,KAAK,6MAAG,SAAM,CAAC,mBAAmB,CAAC,IAAI,CAAC,CAAC;YAC/C,IAAI,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;gBACjC,MAAM,SAAS,GAAG,KAAK,CAAC;gBACxB,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,GAAG;oBACtB,YAAY,EAAE,GAAG,IAAI,CAAA,SAAA,CAAW;oBAChC,QAAQ,iNAAE,OAAA,AAAI,EAAC,GAAG,CAAG,CAAD,oOAAC,AAAS,EAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;iBAC3D,CAAC;aACH;YAED,MAAM,YAAY,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC;YACpD,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CAC/C,iBAAiB,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAC7B,iBAAiB,CAAC,IAAI,CAAC,CAAC;YAC5B,IAAI,QAAQ,IAAI,IAAI,EAAE;gBACpB,OAAO;aACR;2NAED,OAAA,AAAI,EAAC,GAAG,EAAE;gBACR,IAAI,QAAgB,CAAC;gBACrB,MAAM,eAAe,IAAG,uNAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,IAAI,CAAC,CAAC,EAAE,YAAY,CAAC,EAAE,QAAQ,CAAC,CAAC;gBACjE,IAAI,IAAI,CAAC,WAAW,EAAE;oBACpB,QAAQ,IAAG,uNAAA,AAAG,oNACV,MAAA,AAAG,EAAC,IAAI,CAAC,CAAC,oNAAE,MAAA,AAAG,EAAC,QAAQ,oNAAE,MAAA,AAAG,EAAC,eAAe,EAAE,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC;iBACtE,MAAM;oBACL,QAAQ,qNAAG,MAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,IAAI,CAAC,CAAC,EAAE,eAAe,CAAC,EAAE,KAAK,CAAC,CAAC;iBACrD;gBACD,YAAY,CAAC,MAAM,CAAC,eAAe,CAAC,CAAC;gBACrC,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;YACzB,CAAC,CAAC,CAAC;QACL,CAAC,CAAC,CAAC;QACH,IAAI,CAAC,mBAAmB,EAAE,CAAC;IAC7B,CAAC;IAEQ,OAAO,GAAA;QACd,IAAI,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC;QACjB,IAAI,IAAI,CAAC,aAAa,IAAI,IAAI,EAAE;aAC9B,wNAAA,AAAO,EAAC,IAAI,CAAC,aAAa,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;SAClD;IACH,CAAC;IAED;;;;OAIG,CACH,WAAW,CAAC,QAAgB,EAAA;QAC1B,IAAI,CAAC,QAAQ,GAAG,QAAQ,CAAC;IAC3B,CAAC;IAEQ,KAAK,CAAC,UAAU,GAAA;QACvB,0CAA0C;QAC1C,OAAO;YAAC,MAAM,IAAI,CAAC,cAAc,EAAE;SAAC,CAAC,MAAM,CAAC,IAAI,CAAC,aAAa,CAAC,GAAG,EAC9D,CAAC,CAAC,EAAE,AAAC,CAAC;gBAAC,IAAI,EAAE,CAAC,CAAC,YAAY;gBAAE,MAAM,EAAE,CAAC,CAAC,QAAQ;YAAA,CAAC,CAAC,CAAC,CAAC,CAAC;IAC1D,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,YAA2B,EAAA;QACnD,YAAY,GAAG,MAAM,IAAI,CAAC,iBAAiB,CAAC,YAAY,CAAC,CAAC;QAC1D,MAAM,SAAS,GAAG,KAAK,CAAC;QACxB,IAAI,CAAC,aAAa,GAAG,YAAY,CAAC,GAAG,EACjC,CAAC,CAAC,EAAG,AAAD,CAAE;gBAAC,YAAY,EAAE,CAAC,CAAC,IAAI;gBAAE,QAAQ,EAAE,CAAC,CAAC,MAAM,CAAC,QAAQ,CAAC,SAAS,CAAC;YAAA,CAAC,CAAC,CAAC,CAAC;IAC7E,CAAC;IAEQ,SAAS,GAAA;QAChB,OAAO;YACL,cAAc,EAAE,IAAI,CAAC,YAAY;YACjC,UAAU,EAAE,IAAI,CAAC,QAAQ;YACzB,aAAa,EAAE,IAAI,CAAC,WAAW;SAChC,CAAC;IACJ,CAAC;IAED,gBAAA,EAAkB,CAClB,MAAM,CAAU,UAAU,CACtB,GAA+B,EAAE,MAAkB,EAAA;QACrD,OAAO,IAAI,GAAG,CACV,MAAM,CAAC,cAAc,CAAC,EAAE,MAAM,CAAC,UAAU,CAAC,EAAE,MAAM,CAAC,aAAa,CAAC,CAAC,CAAC;IACzE,CAAC;CACF","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 911, "column": 0}, "map": {"version":3,"file":"rmsprop_optimizer.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-core/src/optimizers/rmsprop_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {mul} from '../ops/mul';\nimport {sqrt} from '../ops/sqrt';\nimport {square} from '../ops/square';\nimport {sub} from '../ops/sub';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, Serializable, SerializableConstructor} from '../serialization';\nimport {NamedTensor, NamedTensorMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\n/** @doclink Optimizer */\nexport class RMSPropOptimizer extends Optimizer {\n  /** @nocollapse */\n  static get className() {\n    // Name matters for Python compatibility.\n    // This is a getter instead of a property because when it's a property, it\n    // prevents the entire class from being tree-shaken.\n    return 'RMSProp';\n  }\n  private centered: boolean;\n\n  private accumulatedMeanSquares: OptimizerVariable[] = [];\n  private accumulatedMoments: OptimizerVariable[] = [];\n  private accumulatedMeanGrads: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected decay = 0.9,\n      protected momentum = 0.0, protected epsilon: number = null,\n      centered = false) {\n    super();\n\n    this.centered = centered;\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n    if (learningRate == null) {\n      throw new Error(`learningRate for RMSPropOptimizer must be defined.`);\n    }\n  }\n\n  applyGradients(variableGradients: NamedTensorMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n      if (this.accumulatedMeanSquares[i] == null) {\n        this.accumulatedMeanSquares[i] = {\n          originalName: `${name}/rms`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMoments[i] == null) {\n        this.accumulatedMoments[i] = {\n          originalName: `${name}/momentum`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMeanGrads[i] == null && this.centered) {\n        this.accumulatedMeanGrads[i] = {\n          originalName: `${name}/mg`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;\n      const accumulatedMoments = this.accumulatedMoments[i].variable;\n      tidy(() => {\n        const newAccumulatedMeanSquare =\n            add(mul(accumulatedMeanSquare, this.decay),\n                mul(square(gradient), 1 - this.decay));\n\n        if (this.centered) {\n          const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable;\n          // Centered gradient\n          const newAccumulatedMeanGrad =\n              add(mul(accumulatedMeanGrad, this.decay),\n                  mul(gradient, 1 - this.decay));\n\n          const gradContribution =\n              div(mul(gradient, this.learningRate),\n                  sqrt(\n                      sub(newAccumulatedMeanSquare,\n                          add(square(newAccumulatedMeanGrad), this.epsilon))));\n          const newAccumulatedMoments =\n              add(mul(accumulatedMoments, this.momentum), gradContribution);\n\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMeanGrad.assign(newAccumulatedMeanGrad);\n          accumulatedMoments.assign(newAccumulatedMoments);\n\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        } else {\n          // Plain gradient\n          const newAccumulatedMeanSquare =\n              add(mul(accumulatedMeanSquare, this.decay),\n                  mul(square(gradient), 1 - this.decay));\n\n          const newAccumulatedMoments =\n              add(mul(accumulatedMoments, this.momentum),\n                  div(mul(gradient, this.learningRate),\n                      sqrt(add(newAccumulatedMeanSquare, this.epsilon))));\n\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMoments.assign(newAccumulatedMoments);\n\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        }\n      });\n    });\n    this.incrementIterations();\n  }\n\n  override dispose(): void {\n    if (this.accumulatedMeanSquares != null) {\n      dispose(this.accumulatedMeanSquares.map(v => v.variable));\n    }\n    if (this.accumulatedMeanGrads != null && this.centered) {\n      dispose(this.accumulatedMeanGrads.map(v => v.variable));\n    }\n    if (this.accumulatedMoments != null) {\n      dispose(this.accumulatedMoments.map(v => v.variable));\n    }\n  }\n\n  override async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    const variables: OptimizerVariable[] =\n        [...this.accumulatedMeanSquares, ...this.accumulatedMoments];\n    if (this.centered) {\n      variables.push(...this.accumulatedMeanGrads);\n    }\n    return [await this.saveIterations()].concat(\n        variables.map(v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  override async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount =\n        this.centered ? weightValues.length / 3 : weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedMeanSquares =\n        weightValues.slice(0, variableCount).map(v => ({\n                                                   originalName: v.name,\n                                                   variable: v.tensor.variable(\n                                                       trainable)\n                                                 }));\n    this.accumulatedMoments =\n        weightValues.slice(variableCount, variableCount * 2)\n            .map(v => ({\n                   originalName: v.name,\n                   variable: v.tensor.variable(trainable)\n                 }));\n    if (this.centered) {\n      this.accumulatedMeanGrads =\n          weightValues.slice(variableCount * 2, variableCount * 3)\n              .map(v => ({\n                     originalName: v.name,\n                     variable: v.tensor.variable(trainable)\n                   }));\n    }\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'decay': this.decay,\n      'momentum': this.momentum,\n      'epsilon': this.epsilon,\n      'centered': this.centered\n    };\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config['learningRate'], config['decay'], config['momentum'],\n        config['epsilon'], config['centered']);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;;;AAEH,OAAO,EAAC,MAAM,EAAC,MAAM,WAAW,CAAC;AACjC,OAAO,EAAC,OAAO,EAAE,IAAI,EAAC,MAAM,YAAY,CAAC;AACzC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,IAAI,EAAC,MAAM,aAAa,CAAC;AACjC,OAAO,EAAC,MAAM,EAAC,MAAM,eAAe,CAAC;AACrC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,SAAS,EAAC,MAAM,mBAAmB,CAAC;AAI5C,OAAO,EAAC,SAAS,EAAoB,MAAM,aAAa,CAAC;;;;;;;;;;;AAGnD,MAAO,gBAAiB,oOAAQ,YAAS;IAC7C,gBAAA,EAAkB,CAClB,MAAM,KAAK,SAAS,GAAA;QAClB,yCAAyC;QACzC,0EAA0E;QAC1E,oDAAoD;QACpD,OAAO,SAAS,CAAC;IACnB,CAAC;IAOD,YACc,YAAoB,EAAY,QAAQ,GAAG,EAC3C,WAAW,GAAG,EAAY,UAAkB,IAAI,EAC1D,QAAQ,GAAG,KAAK,CAAA;QAClB,KAAK,EAAE,CAAC;QAHI,IAAA,CAAA,YAAY,GAAZ,YAAY,CAAQ;QAAY,IAAA,CAAA,KAAK,GAAL,KAAK,CAAM;QAC3C,IAAA,CAAA,QAAQ,GAAR,QAAQ,CAAM;QAAY,IAAA,CAAA,OAAO,GAAP,OAAO,CAAe;QANtD,IAAA,CAAA,sBAAsB,GAAwB,EAAE,CAAC;QACjD,IAAA,CAAA,kBAAkB,GAAwB,EAAE,CAAC;QAC7C,IAAA,CAAA,oBAAoB,GAAwB,EAAE,CAAC;QAQrD,IAAI,CAAC,QAAQ,GAAG,QAAQ,CAAC;QAEzB,IAAI,OAAO,IAAI,IAAI,EAAE;YACnB,IAAI,CAAC,OAAO,4MAAG,UAAM,CAAC,OAAO,CAAC,OAAO,EAAE,CAAC;SACzC;QACD,IAAI,YAAY,IAAI,IAAI,EAAE;YACxB,MAAM,IAAI,KAAK,CAAC,CAAA,kDAAA,CAAoD,CAAC,CAAC;SACvE;IACH,CAAC;IAED,cAAc,CAAC,iBAA+C,EAAA;QAC5D,MAAM,aAAa,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CACpD,iBAAiB,CAAC,GAAG,EAAC,IAAI,CAAC,EAAE,AAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAC1C,MAAM,CAAC,IAAI,CAAC,iBAAiB,CAAC,CAAC;QAEnC,aAAa,CAAC,OAAO,CAAC,CAAC,IAAI,EAAE,CAAC,EAAE,EAAE;YAChC,MAAM,KAAK,6MAAG,SAAM,CAAC,mBAAmB,CAAC,IAAI,CAAC,CAAC;YAC/C,MAAM,SAAS,GAAG,KAAK,CAAC;YACxB,IAAI,IAAI,CAAC,sBAAsB,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;gBAC1C,IAAI,CAAC,sBAAsB,CAAC,CAAC,CAAC,GAAG;oBAC/B,YAAY,EAAE,GAAG,IAAI,CAAA,IAAA,CAAM;oBAC3B,QAAQ,EAAE,sNAAA,AAAI,EAAC,GAAG,EAAE,wNAAC,YAAA,AAAS,EAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;iBAC3D,CAAC;aACH;YACD,IAAI,IAAI,CAAC,kBAAkB,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;gBACtC,IAAI,CAAC,kBAAkB,CAAC,CAAC,CAAC,GAAG;oBAC3B,YAAY,EAAE,GAAG,IAAI,CAAA,SAAA,CAAW;oBAChC,QAAQ,iNAAE,OAAA,AAAI,EAAC,GAAG,EAAE,wNAAC,YAAA,AAAS,EAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;iBAC3D,CAAC;aACH;YACD,IAAI,IAAI,CAAC,oBAAoB,CAAC,CAAC,CAAC,IAAI,IAAI,IAAI,IAAI,CAAC,QAAQ,EAAE;gBACzD,IAAI,CAAC,oBAAoB,CAAC,CAAC,CAAC,GAAG;oBAC7B,YAAY,EAAE,GAAG,IAAI,CAAA,GAAA,CAAK;oBAC1B,QAAQ,iNAAE,OAAA,AAAI,EAAC,GAAG,EAAE,wNAAC,YAAA,AAAS,EAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;iBAC3D,CAAC;aACH;YAED,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAO,CAAC,iBAAiB,CAAC,CAAC,CAAC,CAC/C,iBAAiB,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAC7B,iBAAiB,CAAC,IAAI,CAAC,CAAC;YAC5B,IAAI,QAAQ,IAAI,IAAI,EAAE;gBACpB,OAAO;aACR;YAED,MAAM,qBAAqB,GAAG,IAAI,CAAC,sBAAsB,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC;YACtE,MAAM,kBAAkB,GAAG,IAAI,CAAC,kBAAkB,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC;YAC/D,sNAAA,AAAI,EAAC,GAAG,EAAE;gBACR,MAAM,wBAAwB,qNAC1B,MAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,qBAAqB,EAAE,IAAI,CAAC,KAAK,CAAC,oNACtC,MAAA,AAAG,uNAAC,SAAA,AAAM,EAAC,QAAQ,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC;gBAE/C,IAAI,IAAI,CAAC,QAAQ,EAAE;oBACjB,MAAM,mBAAmB,GAAG,IAAI,CAAC,oBAAoB,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC;oBAClE,oBAAoB;oBACpB,MAAM,sBAAsB,qNACxB,MAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,mBAAmB,EAAE,IAAI,CAAC,KAAK,CAAC,MACpC,oNAAA,AAAG,EAAC,QAAQ,EAAE,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC;oBAEvC,MAAM,gBAAgB,qNAClB,MAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,QAAQ,EAAE,IAAI,CAAC,YAAY,CAAC,qNAChC,OAAA,AAAI,oNACA,MAAA,AAAG,EAAC,wBAAwB,oNACxB,MAAA,AAAG,MAAC,0NAAA,AAAM,EAAC,sBAAsB,CAAC,EAAE,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;oBACrE,MAAM,qBAAqB,qNACvB,MAAA,AAAG,GAAC,uNAAA,AAAG,EAAC,kBAAkB,EAAE,IAAI,CAAC,QAAQ,CAAC,EAAE,gBAAgB,CAAC,CAAC;oBAElE,qBAAqB,CAAC,MAAM,CAAC,wBAAwB,CAAC,CAAC;oBACvD,mBAAmB,CAAC,MAAM,CAAC,sBAAsB,CAAC,CAAC;oBACnD,kBAAkB,CAAC,MAAM,CAAC,qBAAqB,CAAC,CAAC;oBAEjD,MAAM,QAAQ,qNAAG,MAAA,AAAG,EAAC,KAAK,EAAE,qBAAqB,CAAC,CAAC;oBACnD,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;iBACxB,MAAM;oBACL,iBAAiB;oBACjB,MAAM,wBAAwB,qNAC1B,MAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,qBAAqB,EAAE,IAAI,CAAC,KAAK,CAAC,GACtC,uNAAA,AAAG,uNAAC,SAAA,AAAM,EAAC,QAAQ,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC;oBAE/C,MAAM,qBAAqB,qNACvB,MAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,kBAAkB,EAAE,IAAI,CAAC,QAAQ,CAAC,EACtC,wNAAA,AAAG,oNAAC,MAAA,AAAG,EAAC,QAAQ,EAAE,IAAI,CAAC,YAAY,CAAC,oNAChC,QAAA,AAAI,oNAAC,MAAA,AAAG,EAAC,wBAAwB,EAAE,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;oBAEhE,qBAAqB,CAAC,MAAM,CAAC,wBAAwB,CAAC,CAAC;oBACvD,kBAAkB,CAAC,MAAM,CAAC,qBAAqB,CAAC,CAAC;oBAEjD,MAAM,QAAQ,qNAAG,MAAA,AAAG,EAAC,KAAK,EAAE,qBAAqB,CAAC,CAAC;oBACnD,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;iBACxB;YACH,CAAC,CAAC,CAAC;QACL,CAAC,CAAC,CAAC;QACH,IAAI,CAAC,mBAAmB,EAAE,CAAC;IAC7B,CAAC;IAEQ,OAAO,GAAA;QACd,IAAI,IAAI,CAAC,sBAAsB,IAAI,IAAI,EAAE;2NACvC,UAAA,AAAO,EAAC,IAAI,CAAC,sBAAsB,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;SAC3D;QACD,IAAI,IAAI,CAAC,oBAAoB,IAAI,IAAI,IAAI,IAAI,CAAC,QAAQ,EAAE;2NACtD,UAAA,AAAO,EAAC,IAAI,CAAC,oBAAoB,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;SACzD;QACD,IAAI,IAAI,CAAC,kBAAkB,IAAI,IAAI,EAAE;2NACnC,UAAA,AAAO,EAAC,IAAI,CAAC,kBAAkB,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;SACvD;IACH,CAAC;IAEQ,KAAK,CAAC,UAAU,GAAA;QACvB,0CAA0C;QAC1C,MAAM,SAAS,GACX,CAAC;eAAG,IAAI,CAAC,sBAAsB,EAAE;eAAG,IAAI,CAAC,kBAAkB;SAAC,CAAC;QACjE,IAAI,IAAI,CAAC,QAAQ,EAAE;YACjB,SAAS,CAAC,IAAI,CAAC,GAAG,IAAI,CAAC,oBAAoB,CAAC,CAAC;SAC9C;QACD,OAAO;YAAC,MAAM,IAAI,CAAC,cAAc,EAAE;SAAC,CAAC,MAAM,CACvC,SAAS,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC;gBAAC,IAAI,EAAE,CAAC,CAAC,YAAY;gBAAE,MAAM,EAAE,CAAC,CAAC,QAAQ;YAAA,CAAC,CAAC,CAAC,CAAC,CAAC;IACxE,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,YAA2B,EAAA;QACnD,YAAY,GAAG,MAAM,IAAI,CAAC,iBAAiB,CAAC,YAAY,CAAC,CAAC;QAC1D,MAAM,aAAa,GACf,IAAI,CAAC,QAAQ,CAAC,CAAC,CAAC,YAAY,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC,YAAY,CAAC,MAAM,GAAG,CAAC,CAAC;QACtE,MAAM,SAAS,GAAG,KAAK,CAAC;QACxB,IAAI,CAAC,sBAAsB,GACvB,YAAY,CAAC,KAAK,CAAC,CAAC,EAAE,aAAa,CAAC,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC;gBACJ,YAAY,EAAE,CAAC,CAAC,IAAI;gBACpB,QAAQ,EAAE,CAAC,CAAC,MAAM,CAAC,QAAQ,CACvB,SAAS,CAAC;aACf,CAAC,CAAC,CAAC;QACjD,IAAI,CAAC,kBAAkB,GACnB,YAAY,CAAC,KAAK,CAAC,aAAa,EAAE,aAAa,GAAG,CAAC,CAAC,CAC/C,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC;gBACJ,YAAY,EAAE,CAAC,CAAC,IAAI;gBACpB,QAAQ,EAAE,CAAC,CAAC,MAAM,CAAC,QAAQ,CAAC,SAAS,CAAC;aACvC,CAAC,CAAC,CAAC;QACjB,IAAI,IAAI,CAAC,QAAQ,EAAE;YACjB,IAAI,CAAC,oBAAoB,GACrB,YAAY,CAAC,KAAK,CAAC,aAAa,GAAG,CAAC,EAAE,aAAa,GAAG,CAAC,CAAC,CACnD,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC;oBACJ,YAAY,EAAE,CAAC,CAAC,IAAI;oBACpB,QAAQ,EAAE,CAAC,CAAC,MAAM,CAAC,QAAQ,CAAC,SAAS,CAAC;iBACvC,CAAC,CAAC,CAAC;SAClB;IACH,CAAC;IAED,SAAS,GAAA;QACP,OAAO;YACL,cAAc,EAAE,IAAI,CAAC,YAAY;YACjC,OAAO,EAAE,IAAI,CAAC,KAAK;YACnB,UAAU,EAAE,IAAI,CAAC,QAAQ;YACzB,SAAS,EAAE,IAAI,CAAC,OAAO;YACvB,UAAU,EAAE,IAAI,CAAC,QAAQ;SAC1B,CAAC;IACJ,CAAC;IAED,gBAAA,EAAkB,CAClB,MAAM,CAAU,UAAU,CACtB,GAA+B,EAAE,MAAkB,EAAA;QACrD,OAAO,IAAI,GAAG,CACV,MAAM,CAAC,cAAc,CAAC,EAAE,MAAM,CAAC,OAAO,CAAC,EAAE,MAAM,CAAC,UAAU,CAAC,EAC3D,MAAM,CAAC,SAAS,CAAC,EAAE,MAAM,CAAC,UAAU,CAAC,CAAC,CAAC;IAC7C,CAAC;CACF","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1093, "column": 0}, "map": {"version":3,"file":"register_optimizers.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-core/src/optimizers/register_optimizers.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2022 Google LLC.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {AdadeltaOptimizer} from './adadelta_optimizer';\nimport {AdagradOptimizer} from './adagrad_optimizer';\nimport {AdamOptimizer} from './adam_optimizer';\nimport {AdamaxOptimizer} from './adamax_optimizer';\nimport {MomentumOptimizer} from './momentum_optimizer';\nimport {RMSPropOptimizer} from './rmsprop_optimizer';\nimport {SGDOptimizer} from './sgd_optimizer';\nimport {registerClass} from '../serialization';\n\nconst OPTIMIZERS = [\n  AdadeltaOptimizer,\n  AdagradOptimizer,\n  AdamOptimizer,\n  AdamaxOptimizer,\n  MomentumOptimizer,\n  RMSPropOptimizer,\n  SGDOptimizer,\n];\n\nexport function registerOptimizers() {\n  for (const optimizer of OPTIMIZERS) {\n    registerClass(optimizer);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;;;AAEH,OAAO,EAAC,iBAAiB,EAAC,MAAM,sBAAsB,CAAC;AACvD,OAAO,EAAC,gBAAgB,EAAC,MAAM,qBAAqB,CAAC;AACrD,OAAO,EAAC,aAAa,EAAC,MAAM,kBAAkB,CAAC;AAC/C,OAAO,EAAC,eAAe,EAAC,MAAM,oBAAoB,CAAC;AACnD,OAAO,EAAC,iBAAiB,EAAC,MAAM,sBAAsB,CAAC;AACvD,OAAO,EAAC,gBAAgB,EAAC,MAAM,qBAAqB,CAAC;AACrD,OAAO,EAAC,YAAY,EAAC,MAAM,iBAAiB,CAAC;AAC7C,OAAO,EAAC,aAAa,EAAC,MAAM,kBAAkB,CAAC;;;;;;;;;AAE/C,MAAM,UAAU,GAAG;wOACjB,oBAAiB;uOACjB,mBAAgB;oOAChB,gBAAa;sOACb,kBAAe;wOACf,oBAAiB;uOACjB,mBAAgB;mOAChB,eAAY;CACb,CAAC;AAEI,SAAU,kBAAkB;IAChC,KAAK,MAAM,SAAS,IAAI,UAAU,CAAE;6NAClC,gBAAA,AAAa,EAAC,SAAS,CAAC,CAAC;KAC1B;AACH,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1147, "column": 0}, "map": {"version":3,"file":"optimizer_constructors.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-core/src/optimizers/optimizer_constructors.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {AdadeltaOptimizer} from './adadelta_optimizer';\nimport {AdagradOptimizer} from './adagrad_optimizer';\nimport {AdamOptimizer} from './adam_optimizer';\nimport {AdamaxOptimizer} from './adamax_optimizer';\nimport {MomentumOptimizer} from './momentum_optimizer';\nimport {RMSPropOptimizer} from './rmsprop_optimizer';\nimport {SGDOptimizer} from './sgd_optimizer';\n\nexport class OptimizerConstructors {\n  /**\n   * Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.\n   *\n   * ```js\n   * // Fit a quadratic function by learning the coefficients a, b, c.\n   * const xs = tf.tensor1d([0, 1, 2, 3]);\n   * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);\n   *\n   * const a = tf.scalar(Math.random()).variable();\n   * const b = tf.scalar(Math.random()).variable();\n   * const c = tf.scalar(Math.random()).variable();\n   *\n   * // y = a * x^2 + b * x + c.\n   * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);\n   * const loss = (pred, label) => pred.sub(label).square().mean();\n   *\n   * const learningRate = 0.01;\n   * const optimizer = tf.train.sgd(learningRate);\n   *\n   * // Train the model.\n   * for (let i = 0; i < 10; i++) {\n   *   optimizer.minimize(() => loss(f(xs), ys));\n   * }\n   *\n   * // Make predictions.\n   * console.log(\n   *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);\n   * const preds = f(xs).dataSync();\n   * preds.forEach((pred, i) => {\n   *   console.log(`x: ${i}, pred: ${pred}`);\n   * });\n   * ```\n   *\n   * @param learningRate The learning rate to use for the SGD algorithm.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static sgd(learningRate: number): SGDOptimizer {\n    return new SGDOptimizer(learningRate);\n  }\n\n  /**\n   * Constructs a `tf.MomentumOptimizer` that uses momentum gradient\n   * descent.\n   *\n   * See\n   * [http://proceedings.mlr.press/v28/sutskever13.pdf](\n   * http://proceedings.mlr.press/v28/sutskever13.pdf)\n   *\n   * @param learningRate The learning rate to use for the Momentum gradient\n   * descent algorithm.\n   * @param momentum The momentum to use for the momentum gradient descent\n   * algorithm.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static momentum(learningRate: number, momentum: number, useNesterov = false):\n      MomentumOptimizer {\n    return new MomentumOptimizer(learningRate, momentum, useNesterov);\n  }\n\n  /**\n   * Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient\n   * descent. This implementation uses plain momentum and is not centered\n   * version of RMSProp.\n   *\n   * See\n   * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](\n   * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n   *\n   * @param learningRate The learning rate to use for the RMSProp gradient\n   * descent algorithm.\n   * @param decay The discounting factor for the history/coming gradient.\n   * @param momentum The momentum to use for the RMSProp gradient descent\n   * algorithm.\n   * @param epsilon Small value to avoid zero denominator.\n   * @param centered If true, gradients are normalized by the estimated\n   * variance of the gradient.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static rmsprop(\n      learningRate: number, decay = .9, momentum = 0.0, epsilon: number = null,\n      centered = false): RMSPropOptimizer {\n    return new RMSPropOptimizer(\n        learningRate, decay, momentum, epsilon, centered);\n  }\n\n  /**\n   * Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adam gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adam(\n      learningRate = 0.001, beta1 = 0.9, beta2 = 0.999,\n      epsilon: number = null): AdamOptimizer {\n    return new AdamOptimizer(learningRate, beta1, beta2, epsilon);\n  }\n\n  /**\n   * Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.\n   * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)\n   *\n   * @param learningRate The learning rate to use for the Adadelta gradient\n   * descent algorithm.\n   * @param rho The learning rate decay over each update.\n   * @param epsilon A constant epsilon used to better condition the grad\n   * update.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adadelta(learningRate = .001, rho = .95, epsilon: number = null):\n      AdadeltaOptimizer {\n    return new AdadeltaOptimizer(learningRate, rho, epsilon);\n  }\n\n  /**\n   * Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adamax gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   * @param decay The learning rate decay over each update.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adamax(\n      learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, epsilon: number = null,\n      decay = 0.0): AdamaxOptimizer {\n    return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n  }\n\n  /**\n   * Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.\n   * See\n   * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](\n   * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n   * or\n   * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](\n   * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n   *\n   * @param learningRate The learning rate to use for the Adagrad gradient\n   * descent algorithm.\n   * @param initialAccumulatorValue Starting value for the accumulators, must be\n   * positive.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adagrad(learningRate: number, initialAccumulatorValue = 0.1):\n      AdagradOptimizer {\n    return new AdagradOptimizer(learningRate, initialAccumulatorValue);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;;;AAEH,OAAO,EAAC,iBAAiB,EAAC,MAAM,sBAAsB,CAAC;AACvD,OAAO,EAAC,gBAAgB,EAAC,MAAM,qBAAqB,CAAC;AACrD,OAAO,EAAC,aAAa,EAAC,MAAM,kBAAkB,CAAC;AAC/C,OAAO,EAAC,eAAe,EAAC,MAAM,oBAAoB,CAAC;AACnD,OAAO,EAAC,iBAAiB,EAAC,MAAM,sBAAsB,CAAC;AACvD,OAAO,EAAC,gBAAgB,EAAC,MAAM,qBAAqB,CAAC;AACrD,OAAO,EAAC,YAAY,EAAC,MAAM,iBAAiB,CAAC;;;;;;;;AAEvC,MAAO,qBAAqB;IAChC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;OAoCG,CACH,MAAM,CAAC,GAAG,CAAC,YAAoB,EAAA;QAC7B,OAAO,mOAAI,eAAY,CAAC,YAAY,CAAC,CAAC;IACxC,CAAC;IAED;;;;;;;;;;;;;;OAcG,CACH,MAAM,CAAC,QAAQ,CAAC,YAAoB,EAAE,QAAgB,EAAE,WAAW,GAAG,KAAK,EAAA;QAEzE,OAAO,wOAAI,oBAAiB,CAAC,YAAY,EAAE,QAAQ,EAAE,WAAW,CAAC,CAAC;IACpE,CAAC;IAED;;;;;;;;;;;;;;;;;;;OAmBG,CACH,MAAM,CAAC,OAAO,CACV,YAAoB,EAAE,KAAK,GAAG,EAAE,EAAE,QAAQ,GAAG,GAAG,EAAE,UAAkB,IAAI,EACxE,QAAQ,GAAG,KAAK,EAAA;QAClB,OAAO,uOAAI,mBAAgB,CACvB,YAAY,EAAE,KAAK,EAAE,QAAQ,EAAE,OAAO,EAAE,QAAQ,CAAC,CAAC;IACxD,CAAC;IAED;;;;;;;;;;;OAWG,CACH,MAAM,CAAC,IAAI,CACP,YAAY,GAAG,KAAK,EAAE,KAAK,GAAG,GAAG,EAAE,KAAK,GAAG,KAAK,EAChD,UAAkB,IAAI,EAAA;QACxB,OAAO,oOAAI,gBAAa,CAAC,YAAY,EAAE,KAAK,EAAE,KAAK,EAAE,OAAO,CAAC,CAAC;IAChE,CAAC;IAED;;;;;;;;;;;OAWG,CACH,MAAM,CAAC,QAAQ,CAAC,YAAY,GAAG,IAAI,EAAE,GAAG,GAAG,GAAG,EAAE,UAAkB,IAAI,EAAA;QAEpE,OAAO,wOAAI,oBAAiB,CAAC,YAAY,EAAE,GAAG,EAAE,OAAO,CAAC,CAAC;IAC3D,CAAC;IAED;;;;;;;;;;;;OAYG,CACH,MAAM,CAAC,MAAM,CACT,YAAY,GAAG,KAAK,EAAE,KAAK,GAAG,GAAG,EAAE,KAAK,GAAG,KAAK,EAAE,UAAkB,IAAI,EACxE,KAAK,GAAG,GAAG,EAAA;QACb,OAAO,sOAAI,kBAAe,CAAC,YAAY,EAAE,KAAK,EAAE,KAAK,EAAE,OAAO,EAAE,KAAK,CAAC,CAAC;IACzE,CAAC;IAED;;;;;;;;;;;;;;;OAeG,CACH,MAAM,CAAC,OAAO,CAAC,YAAoB,EAAE,uBAAuB,GAAG,GAAG,EAAA;QAEhE,OAAO,uOAAI,mBAAgB,CAAC,YAAY,EAAE,uBAAuB,CAAC,CAAC;IACrE,CAAC;CACF","ignoreList":[0],"debugId":null}}]
}